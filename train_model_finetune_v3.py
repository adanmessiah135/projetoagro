import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from datetime import datetime
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.metrics import confusion_matrix, classification_report

# ============================================================
# üîß CONFIGURA√á√ïES GERAIS
# ============================================================
BASE_DIR = "dataset"
IMG_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 30

# --- CAMINHOS DE SA√çDA ---
OUTPUT_DIR = "ml"
MODEL_DIR = os.path.join(OUTPUT_DIR, "models")
MODEL_PATH = os.path.join(MODEL_DIR, "cana_model_v5_mobilenet.keras")
ACCURACY_PLOT_PATH = os.path.join(OUTPUT_DIR, "training_accuracy_mobilenet.png")
CM_PLOT_PATH = os.path.join(OUTPUT_DIR, "confusion_matrix_mobilenet.png")

# ‚ú® AJUSTE: Cria os diret√≥rios de sa√≠da se n√£o existirem ‚ú®
os.makedirs(MODEL_DIR, exist_ok=True)

print(f"\nüöÄ Iniciando fine-tuning com MobileNetV2 (ImageNet)...")
print(f"Salvando modelo em: {MODEL_PATH}")
print(f"Salvando gr√°ficos em: {OUTPUT_DIR}/")

# ============================================================
# üìä GERADORES DE DADOS
# ============================================================
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input

# 1. Gerador para TREINO (com Data Augmentation e pr√©-processamento)
train_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,  # <-- ESTA √â A CORRE√á√ÉO
    rotation_range=25,
    zoom_range=0.25,
    width_shift_range=0.1,
    height_shift_range=0.1,
    brightness_range=[0.8, 1.2],
    horizontal_flip=True,
    fill_mode="nearest",
    validation_split=0.2  # Separa 20% para valida√ß√£o
)

# 2. Gerador para VALIDA√á√ÉO (APENAS pr√©-processamento, SEM Augmentation)
val_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,  # <-- CORRE√á√ÉO
    validation_split=0.2  # Deve ser o mesmo split
)

# 3. Cria os geradores a partir das pastas
train_generator = train_datagen.flow_from_directory(
    BASE_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    color_mode="rgb",
    class_mode="categorical",
    subset="training"  # Pega a fatia de treino
)

val_generator = val_datagen.flow_from_directory(
    BASE_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    color_mode="rgb",
    class_mode="categorical",
    subset="validation",  # Pega a fatia de valida√ß√£o
    shuffle=False
)

# ============================================================
# ‚ÄºÔ∏è ‚ÄºÔ∏è BLOCO DE VERIFICA√á√ÉO ‚ÄºÔ∏è ‚ÄºÔ∏è
# ============================================================
# ‚ú® ADICIONADO: Verifica se as pastas foram lidas corretamente
print("\n" + "="*50)
print("VERIFICANDO PASTAS E CLASSES:")
print("Classes encontradas:", train_generator.class_indices)
print(f"Total de imagens de treino: {train_generator.samples}")
print(f"Total de imagens de valida√ß√£o: {val_generator.samples}")
print(f"Total de classes detectadas: {train_generator.num_classes}")
print("="*50 + "\n")

# Se "Total de classes detectadas" n√£o for 5, PARE E ARRUME AS PASTAS.
# ============================================================

# ============================================================
# üß† MODELO BASE (MobileNetV2 PR√â-TREINADO)
# ============================================================
base_model = MobileNetV2(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False  # Congela as camadas iniciais

x = base_model.output
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.5)(x)  # Regulariza√ß√£o forte
x = layers.Dense(256, activation="relu")(x)
x = layers.Dropout(0.3)(x)  # Mais regulariza√ß√£o
predictions = layers.Dense(train_generator.num_classes, activation="softmax")(x)

model = models.Model(inputs=base_model.input, outputs=predictions)

model.compile(
    optimizer=optimizers.Adam(learning_rate=1e-4), # Volte para 1e-4
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

# ============================================================
# üìà CALLBACKS
# ============================================================
checkpoint = ModelCheckpoint(
    MODEL_PATH,
    monitor="val_accuracy",
    save_best_only=True,
    verbose=1
)
early_stop = EarlyStopping(
    monitor="val_loss",
    patience=5,  # Para 5 √©pocas sem melhoria na perda de valida√ß√£o
    restore_best_weights=True
)

# ============================================================
# üöÄ TREINAMENTO (APENAS ETAPA 1)
# ============================================================
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=EPOCHS,
    callbacks=[checkpoint, early_stop]
)

# ============================================================
# üìà RELAT√ìRIO DE RESULTADOS
# ============================================================
plt.figure(figsize=(10, 5))
plt.plot(history.history["accuracy"], label="Treino")
plt.plot(history.history["val_accuracy"], label="Valida√ß√£o")
plt.title("Acur√°cia do Modelo (MobileNetV2 Fine-Tuning)")
plt.xlabel("√âpocas")
plt.ylabel("Acur√°cia")
plt.legend()
plt.tight_layout()
plt.savefig(ACCURACY_PLOT_PATH) # Salva no caminho definido
plt.show()

# ============================================================
# üß† RELAT√ìRIO AUTOM√ÅTICO DE CONFIAN√áA
# ============================================================
# Encontra a melhor acur√°cia de valida√ß√£o alcan√ßada
final_val_acc = max(history.history["val_accuracy"]) * 100
if final_val_acc >= 80:
    interpretacao = "Alta confian√ßa ‚úÖ ‚Äî modelo muito consistente."
elif final_val_acc >= 65:
    interpretacao = "Confian√ßa m√©dia üî∂ ‚Äî modelo razo√°vel, pode melhorar com mais dados."
else:
    interpretacao = "Confian√ßa baixa ‚ö†Ô∏è ‚Äî modelo precisa de mais exemplos ou ajuste de classes."

print(f"\nüìä Acur√°cia final de valida√ß√£o: {final_val_acc:.2f}%")
print(f"üí¨ Interpreta√ß√£o: {interpretacao}")

# ============================================================
# üìâ MATRIZ DE CONFUS√ÉO E RELAT√ìRIO POR CLASSE
# ============================================================
print("\nüìä Gerando matriz de confus√£o e relat√≥rio por classe...")

# Predi√ß√µes no conjunto de valida√ß√£o
y_pred = model.predict(val_generator)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = val_generator.classes
class_labels = list(val_generator.class_indices.keys())

# Matriz de confus√£o normalizada
cm = confusion_matrix(y_true, y_pred_classes, normalize="true")
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, cmap="Blues", fmt=".2f", xticklabels=class_labels, yticklabels=class_labels)
plt.title("Matriz de Confus√£o Normalizada (Valida√ß√£o)")
plt.xlabel("Predito")
plt.ylabel("Real")
plt.tight_layout()
plt.savefig(CM_PLOT_PATH) # Salva no caminho definido
plt.show()

# Relat√≥rio detalhado
report = classification_report(y_true, y_pred_classes, target_names=class_labels)
print("\nüìã Relat√≥rio de Classifica√ß√£o:\n")
print(report)

print(f"\n‚úÖ Fine-tuning conclu√≠do e salvo em: {MODEL_PATH}")
print("üß† Classes detectadas:", class_labels)